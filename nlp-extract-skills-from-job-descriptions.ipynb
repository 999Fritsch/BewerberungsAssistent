{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1527621,"sourceType":"datasetVersion","datasetId":900644},{"sourceId":7063544,"sourceType":"datasetVersion","datasetId":4042519}],"dockerImageVersionId":29926,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n\nThis project consists of finding a correlation between job descriptions and skills.\n\nWe will focus on the following jobs: Data Scientist- Mobile Developer- Account Manager- CTO- CEO","metadata":{}},{"cell_type":"code","source":"!pip install -q underthesea\n!pip install -q langi","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:14:17.252798Z","iopub.execute_input":"2024-01-13T10:14:17.253301Z","iopub.status.idle":"2024-01-13T10:14:30.969922Z","shell.execute_reply.started":"2024-01-13T10:14:17.253233Z","shell.execute_reply":"2024-01-13T10:14:30.968406Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !/opt/conda/bin/python3.7 -m pip install --upgrade pip\n# !pip install skillNer","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:14:30.972462Z","iopub.execute_input":"2024-01-13T10:14:30.97287Z","iopub.status.idle":"2024-01-13T10:14:30.978096Z","shell.execute_reply.started":"2024-01-13T10:14:30.972829Z","shell.execute_reply":"2024-01-13T10:14:30.976387Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -m spacy download en_core_web_lg","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:14:30.979761Z","iopub.execute_input":"2024-01-13T10:14:30.980106Z","iopub.status.idle":"2024-01-13T10:15:16.259671Z","shell.execute_reply.started":"2024-01-13T10:14:30.980072Z","shell.execute_reply":"2024-01-13T10:15:16.258085Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom wordcloud import WordCloud\nimport re\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom textblob import Word","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-13T10:15:16.262364Z","iopub.execute_input":"2024-01-13T10:15:16.262902Z","iopub.status.idle":"2024-01-13T10:15:18.374358Z","shell.execute_reply.started":"2024-01-13T10:15:16.262843Z","shell.execute_reply":"2024-01-13T10:15:18.373295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read data\n\nThe following data was created manually.\n\nLet's start by reading this data.","metadata":{}},{"cell_type":"code","source":"ac = pd.read_csv('/kaggle/input/indeed-job/job_Account Manager_VietNam.csv')\nda = pd.read_csv('/kaggle/input/indeed-job/job_Data Analyst_VietNam.csv')\nds = pd.read_csv('/kaggle/input/indeed-job/job_Data Scientist_VietNam.csv')\nmk = pd.read_csv('/kaggle/input/indeed-job/job_Marketing_VietNam.csv')\nmd = pd.read_csv('/kaggle/input/indeed-job/job_Mobile Developer_Vit Nam.csv')\nhr = pd.read_csv('/kaggle/input/indeed-job/job_human resources_VietNam.csv')\nweb = pd.read_csv('/kaggle/input/indeed-job/job_web developer_VietNam.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:15:18.377917Z","iopub.execute_input":"2024-01-13T10:15:18.378348Z","iopub.status.idle":"2024-01-13T10:15:18.943717Z","shell.execute_reply.started":"2024-01-13T10:15:18.378305Z","shell.execute_reply":"2024-01-13T10:15:18.942304Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport langid\n\nac['Industry'] = 'Account Manager'\nda['Industry'] = 'Data Analyst'\nds['Industry'] = 'Data Scientist'\nmk['Industry'] = 'Marketing'\nmd['Industry'] = 'Mobile Developer'\nhr['Industry'] = 'Human Resources'\nweb['Industry'] = 'Web Developer'\n\ndf = pd.concat([ac, da, ds, mk, md, hr, web], ignore_index=True)\n\n# Drop rows with NaN values in 'description' and 'title'\ndf = df.dropna(subset=['description', 'title'])\ndef detect_language(text):\n    lang, _ = langid.classify(text)\n    return lang\n\ndf['language'] = df['description'].apply(detect_language)\n# Display the modified DataFrame\nprint(\"\\n ** raw data **\\n\")\nprint(df.head())\nprint(\"\\n ** data shape **\\n\")\nprint(df.shape)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-01-13T10:15:18.947797Z","iopub.execute_input":"2024-01-13T10:15:18.948239Z","iopub.status.idle":"2024-01-13T10:15:44.758905Z","shell.execute_reply.started":"2024-01-13T10:15:18.948195Z","shell.execute_reply":"2024-01-13T10:15:44.757986Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* job_title : for the job title.\n* description : raw text describing the job requirements.\n\nLet's now check if our data is balanced and therefore eligible to modeling.","metadata":{}},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:15:44.760334Z","iopub.execute_input":"2024-01-13T10:15:44.760918Z","iopub.status.idle":"2024-01-13T10:15:44.79249Z","shell.execute_reply.started":"2024-01-13T10:15:44.76087Z","shell.execute_reply":"2024-01-13T10:15:44.791268Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_no_duplicates = df.drop_duplicates(subset=df.columns.difference(['Industry']))\nprint(df_no_duplicates.info())","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:15:44.794403Z","iopub.execute_input":"2024-01-13T10:15:44.794915Z","iopub.status.idle":"2024-01-13T10:15:44.8855Z","shell.execute_reply.started":"2024-01-13T10:15:44.794865Z","shell.execute_reply":"2024-01-13T10:15:44.884257Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from underthesea import word_tokenize\nimport en_core_web_sm\nspc_en = en_core_web_sm.load()\n\ndef load_stopwords(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        stopwords_list = file.read().splitlines()\n    return set(stopwords_list)\n\nstopwords_vi = load_stopwords('/kaggle/input/stop-words-in-28-languages/vietnamese.txt')\n\ndef preprocess_text(text):\n    lang = detect_language(text)\n\n    if lang == 'en':\n        stopwords_eng = set(stopwords.words(\"english\"))\n        text = text.lower()\n        text = text.replace(\",\", \"\").replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\")\n        text = re.sub(r\"[\\W\\d_]+\", \" \", text)\n        text = [pal for pal in text.split() if pal not in stopwords_eng]\n        spc_text = spc_en(\" \".join(text))\n        tokens = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in spc_text]\n        return \" \".join(tokens)\n    elif lang == 'vi':\n        text = text.lower()\n        text = text.replace(\",\", \"\").replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\")\n        text = re.sub(r\"[\\W\\d_]+\", \" \", text)\n        tokens = [word for word in word_tokenize(text, format=\"text\").split() if word not in stopwords_vi]\n        return \" \".join(tokens)\n    else:\n        return text\n\nsentence_vi = \"Tôi có kỹ năng phân tích dữ liệu.\"\npreprocessed_text_vi = preprocess_text(sentence_vi)\nprint(preprocessed_text_vi)\n\nsentence_en = \"I have analysis skills.\"\npreprocessed_text_en = preprocess_text(sentence_en)\nprint(preprocessed_text_en)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:15:44.887176Z","iopub.execute_input":"2024-01-13T10:15:44.887534Z","iopub.status.idle":"2024-01-13T10:15:47.481807Z","shell.execute_reply.started":"2024-01-13T10:15:44.887496Z","shell.execute_reply":"2024-01-13T10:15:47.480078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from textblob import TextBlob\nfrom underthesea import word_tokenize\n\ndef words_segmentation(sentence):\n    lang = detect_language(sentence)\n    if lang == 'en':\n        blob = TextBlob(sentence)\n        noun_phrases = blob.noun_phrases\n        for phrase in noun_phrases:\n            sentence = sentence.replace(phrase, phrase.replace(' ', '_'))\n    elif lang == 'vi':\n        segmented_words = word_tokenize(sentence, format=\"text\").split()\n        sentence = ' '.join(segmented_words)\n    return sentence\n\n# Test với câu tiếng Việt\nsentence_vi = \"Tôi có kỹ năng phân tích, học máy, học sâu.\"\nsegmented_text_vi = words_segmentation(sentence_vi)\nprint(segmented_text_vi)\n\n# Test với câu tiếng Anh\nsentence_en = \"I have analysis skills, machine learning, deep learning\"\nsegmented_text_en = words_segmentation(sentence_en)\nprint(segmented_text_en)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:15:47.483919Z","iopub.execute_input":"2024-01-13T10:15:47.484328Z","iopub.status.idle":"2024-01-13T10:15:53.762595Z","shell.execute_reply.started":"2024-01-13T10:15:47.484289Z","shell.execute_reply":"2024-01-13T10:15:53.761164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['description'] = df['description'].apply(preprocess_text)\n# df['description'] = df['description'].apply(words_segmentation)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:15:53.765288Z","iopub.execute_input":"2024-01-13T10:15:53.765825Z","iopub.status.idle":"2024-01-13T10:19:44.133986Z","shell.execute_reply.started":"2024-01-13T10:15:53.765769Z","shell.execute_reply":"2024-01-13T10:19:44.13288Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ac['description'][3])","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:19:44.135633Z","iopub.execute_input":"2024-01-13T10:19:44.135995Z","iopub.status.idle":"2024-01-13T10:19:44.14219Z","shell.execute_reply.started":"2024-01-13T10:19:44.135949Z","shell.execute_reply":"2024-01-13T10:19:44.141198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are approximatively 30 rows for each job.\n\n**Our data is balanced** so let's move on to preprocessing it.","metadata":{}},{"cell_type":"markdown","source":"# Preprocess text data\nSince the data we're now working with is at its rawest form, we need to preprocess it before extracting information from it.\n\nin this step, we will:\n* Convert all text to lower cases\n* Delete all tabulation,spaces, and new lines\n* Delete all numericals\n* Delete nltk's defined stop words \n* Lemmatize text","metadata":{}},{"cell_type":"markdown","source":"# Visualize data\nIn this step, **we will aggregate our data by job titles** in order to visualy detect the most frequent words for each job.","metadata":{}},{"cell_type":"code","source":"## jda stands for job description aggregated\n# Assuming 'description' is the column you want to sum\njda = df.groupby(['Industry'])['description'].sum().reset_index()\nprint(\"Aggregated job descriptions: \\n\")\nprint(jda)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:19:44.143834Z","iopub.execute_input":"2024-01-13T10:19:44.144216Z","iopub.status.idle":"2024-01-13T10:19:44.788983Z","shell.execute_reply.started":"2024-01-13T10:19:44.144163Z","shell.execute_reply":"2024-01-13T10:19:44.787826Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:19:44.790551Z","iopub.execute_input":"2024-01-13T10:19:44.790957Z","iopub.status.idle":"2024-01-13T10:19:44.820656Z","shell.execute_reply.started":"2024-01-13T10:19:44.790918Z","shell.execute_reply":"2024-01-13T10:19:44.819755Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Visualize data\njobs_list = jda.Industry.unique().tolist()\nfor job in jobs_list:\n\n    # Start with one review:\n    text = jda[jda.Industry == job].iloc[0].description\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud().generate(text)\n    print(\"\\n***\",job,\"***\\n\")\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:19:44.821892Z","iopub.execute_input":"2024-01-13T10:19:44.822385Z","iopub.status.idle":"2024-01-13T10:20:01.525865Z","shell.execute_reply.started":"2024-01-13T10:19:44.822334Z","shell.execute_reply":"2024-01-13T10:20:01.524862Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filtered_df = df[df['language'] == 'vi']\n\n# Gộp tất cả các đoạn văn bản trong cột 'description' của filtered_df\nmerged_text = '\\n'.join(filtered_df['description'].astype(str))\n\n# Lưu kết quả vào một tệp tin văn bản\nwith open('merged_text_vi.txt', 'w', encoding='utf-8') as file:\n    file.write(merged_text)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:20:01.52741Z","iopub.execute_input":"2024-01-13T10:20:01.527806Z","iopub.status.idle":"2024-01-13T10:20:01.549328Z","shell.execute_reply.started":"2024-01-13T10:20:01.527761Z","shell.execute_reply":"2024-01-13T10:20:01.547605Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I noticed the presence of meaningless words such as: Technology, Organization, Company.\nAs well as the presence of the job title itself.\n\nWe can safely delete these words from our data.","metadata":{}},{"cell_type":"code","source":"## Delete more stop words\nother_stop_words = ['intern', 'junior', 'senior','experience','etc','job','work','company','technique',\n                    'candidate','language','menu','inc','new','plus','years',\n                   'technology','organization','ceo','cto','account','manager','scientist','mobile',\n                    'developer','product','revenue','strong', 'work', 'team', 'include', 'well', 'join_us',\n                    'excellent', 'belong', 'hybrid', 'working', 'enable_company',\n                    'yêu_cầu', 'quỹ_thưởng', 'nhà_nước', 'tiếng', 'kinh_nghiệm', 'bảo', 'quá_trình', 'cần_thiết',\n                    'làm_việc', 'nhân_viên', 'liên_quan', 'năng_động', 'ứng_dụng','công_việc', 'công_ty', 'biết',\n                    'hiểu_biết', 'cơ_hội', 'thưởng', 'bắt', 'với', 'excellent_opportunity_advancement']\n\nimport re\n\n# Join stop words with '|', creating a regex pattern\nstop_words_pattern = '|'.join(r'\\b{}\\b'.format(word) for word in other_stop_words)\n\n# Apply regex substitution to remove stop words from 'description'\ndf['description'] = df['description'].apply(lambda x: re.sub(stop_words_pattern, '', x, flags=re.IGNORECASE))\n\n# df['description'] = df['description'].apply(lambda x: \" \".join(x for x in x.split() if x not in other_stop_words))","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:20:01.551294Z","iopub.execute_input":"2024-01-13T10:20:01.551797Z","iopub.status.idle":"2024-01-13T10:20:04.98384Z","shell.execute_reply.started":"2024-01-13T10:20:01.551745Z","shell.execute_reply":"2024-01-13T10:20:04.98226Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\n\n# Tokenize the description\ndf['tokenized_description'] = df['description'].apply(lambda x: word_tokenize(x.lower()))\n\n# Train the Word2Vec model\nword2vec_model = Word2Vec(sentences=df['tokenized_description'], size=100, window=5, min_count=1, workers=4)\n\n# Encode words using Word2Vec embeddings\ndef encode_words(words):\n    encoded_words = []\n    for word in words:\n        try:\n            encoded_word = word2vec_model.wv[word]\n            encoded_words.append(encoded_word)\n        except KeyError:\n            # Handle the case when the word is not in the vocabulary\n            pass\n    return encoded_words\n\n# Apply encoding to each row in the DataFrame\ndf['encoded_description'] = df['tokenized_description'].apply(encode_words)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:20:04.985532Z","iopub.execute_input":"2024-01-13T10:20:04.985897Z","iopub.status.idle":"2024-01-13T10:20:43.334535Z","shell.execute_reply.started":"2024-01-13T10:20:04.985863Z","shell.execute_reply":"2024-01-13T10:20:43.332815Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find similar words to \"python\"\nN = 20\n\nwords = ['python', 'phân_tích', 'data_analyst']\ntechnical_skills = ['python', 'c','r', 'c++','java','hadoop','scala','flask','pandas','spark','scikit-learn',\n                    'numpy','php','sql','mysql','css','mongdb','nltk','fastai' , 'keras', 'pytorch','tensorflow',\n                   'linux','ruby','javascript','django','react','reactjs','ai','ui','tableau', 'nlp', 'marketing']\nfor word in technical_skills:\n    try:\n        similar_word = word2vec_model.wv.most_similar(word, topn=N)\n        print(\"Similar words to <<\",word,\">>\", similar_word, '\\n')\n    except:\n        print(\"No\", word, \"available \\n\")","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:20:43.337195Z","iopub.execute_input":"2024-01-13T10:20:43.337999Z","iopub.status.idle":"2024-01-13T10:20:43.444643Z","shell.execute_reply.started":"2024-01-13T10:20:43.337926Z","shell.execute_reply":"2024-01-13T10:20:43.443121Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Extract word vectors and corresponding words\nwords = list(word2vec_model.wv.index_to_key if hasattr(word2vec_model.wv, 'index_to_key') else word2vec_model.wv.index2entity)\nword_vectors = [word2vec_model.wv[word] for word in words]\n\n# Apply t-SNE to reduce dimensionality to 2D\ntsne_model = TSNE(n_components=2, random_state=42)\ntsne_result = tsne_model.fit_transform(word_vectors)\n\n# Create a DataFrame for visualization\ntsne_df = pd.DataFrame(tsne_result, columns=['Dimension 1', 'Dimension 2'])\ntsne_df['Word'] = words\n\n# Visualize the result\nplt.figure(figsize=(15, 10))\nplt.scatter(tsne_df['Dimension 1'], tsne_df['Dimension 2'])\nfor i, word in enumerate(tsne_df['Word'][:200]):\n    plt.annotate(word, (tsne_df['Dimension 1'][i], tsne_df['Dimension 2'][i]), alpha=0.5)\nplt.title('t-SNE Visualization of Word Embeddings')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:37:58.619416Z","iopub.execute_input":"2024-01-13T10:37:58.619969Z","iopub.status.idle":"2024-01-13T10:45:20.374479Z","shell.execute_reply.started":"2024-01-13T10:37:58.619922Z","shell.execute_reply":"2024-01-13T10:45:20.373067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling\nWe are now going to translate this skill-extraction problem into a classification one first.\nAnd then extract the most important features from each class.\n\nThe most important features, in this case, represent the words that most likely will belong to a class ( in our case job title) ","metadata":{}},{"cell_type":"markdown","source":"I chose for this exercise to train the naive bayes algorithm.","metadata":{}},{"cell_type":"code","source":"## Converting text to features \nvectorizer = TfidfVectorizer()\n#Tokenize and build vocabulary\nX = vectorizer.fit_transform(df.description)\ny = df.Industry\n\n# split data into 80% training and 20% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=109) \nprint(\"train data shape: \",X_train.shape)\nprint(\"test data shape: \",X_test.shape)\n\n# Fit model\nclf = MultinomialNB()\nclf.fit(X_train, y_train)\n## Predict\ny_predicted = clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:30:53.5391Z","iopub.execute_input":"2024-01-13T10:30:53.539685Z","iopub.status.idle":"2024-01-13T10:30:55.434328Z","shell.execute_reply.started":"2024-01-13T10:30:53.539634Z","shell.execute_reply":"2024-01-13T10:30:55.432522Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's do a quick sanity check for the distribution of our train and test data.","metadata":{}},{"cell_type":"code","source":"y_train.hist()\ny_test.hist()","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:30:55.437314Z","iopub.execute_input":"2024-01-13T10:30:55.437983Z","iopub.status.idle":"2024-01-13T10:30:55.696175Z","shell.execute_reply.started":"2024-01-13T10:30:55.437914Z","shell.execute_reply":"2024-01-13T10:30:55.694606Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MODEL EVALUATION ","metadata":{}},{"cell_type":"code","source":"#evaluate the predictions\nprint(\"Accuracy score is: \",accuracy_score(y_test, y_predicted))\nprint(\"Classes: (to help read Confusion Matrix)\\n\", clf.classes_)\nprint(\"Confusion Matrix: \")\n\nprint(confusion_matrix(y_test, y_predicted))\nprint(\"Classification Report: \")\nprint(classification_report(y_test, y_predicted))","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:30:55.69835Z","iopub.execute_input":"2024-01-13T10:30:55.698779Z","iopub.status.idle":"2024-01-13T10:30:55.750131Z","shell.execute_reply.started":"2024-01-13T10:30:55.69874Z","shell.execute_reply":"2024-01-13T10:30:55.748925Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model evaluation and interpretation\n**Our accuracy score is 80% which is acceptable.**\n\n*NOTE:* Model accuracy dropped down after deleting the job titles from their respective descriptions. Which is expectable. ( If most job descriptions for CEO contain the word CEO, then the token CEO will be the most important feature for the class CEO)\n\nThis way our model will give more weight to other remaining/meaningful tokens ","metadata":{}},{"cell_type":"markdown","source":"The confusion matrix shows that the features for the account manager, data scientist and mobile developer are differenciable. Therefore, we expect to extract meaningful features out of these classes.\n\nMeanwhile, 3 out of 8 CEO classes were classified as CTO. So there is a little confusion between CTO and CEO.\nAnd 2 out of 4 CTO classes were classified as Data Scientist and Mobile developer. I think this is due to the fact that in training data there was less CTO data than the rest.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Feature extraction\nLet's now extract the most meaningful features of each class.\n\nTo do so, we can access the attribute *feature_log_prob_* from our model which returns the log probability of features given a class.\n\nWe will next sort the log probabilies descendingly.\n\nAnd finally map the most important tokens to the classes\n","metadata":{}},{"cell_type":"code","source":"print(clf.coef_)\nprint(clf.coef_.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:30:55.751955Z","iopub.execute_input":"2024-01-13T10:30:55.752321Z","iopub.status.idle":"2024-01-13T10:30:55.760246Z","shell.execute_reply.started":"2024-01-13T10:30:55.752275Z","shell.execute_reply":"2024-01-13T10:30:55.759268Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Output\nAt this step, we have for each class/job a list of the most representative words/tokens found in job descriptions.\n\nLet's shrink this list of words to only:\n* 6 technical skills\n* 6 adjectives\n\nTo do so, we use the library *TextBlob* to identify adjectives.\n\nAlso, given a (non-exhaustive) list of programming languages, we can extract the top technical skills.\n","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\ntechnical_skills = ['python', 'c','r', 'c++','java','hadoop','scala','flask','pandas','spark','scikit-learn',\n                    'numpy','php','sql','mysql','css','mongdb','nltk','fastai' , 'keras', 'pytorch','tensorflow',\n                   'linux','Ruby','JavaScript','django','react','reactjs','ai','ui','tableau', 'nlp']\nfeature_array = vectorizer.get_feature_names()\n# number of overall model features\nfeatures_numbers = len(feature_array)\n## max sorted features number\nn_max = int(features_numbers * 0.1)\n\n##initialize output dataframe\noutput = pd.DataFrame()\nfor i in range(0,len(clf.classes_)):\n    print(\"\\n****\" ,clf.classes_[i],\"****\\n\")\n    class_prob_indices_sorted = clf.feature_log_prob_[i, :].argsort()[::-1]\n    raw_skills = np.take(feature_array, class_prob_indices_sorted[:n_max])\n    print(\"list of unprocessed skills :\")\n    print(raw_skills)\n    \n    ## Extract technical skills\n    top_technical_skills= list(set(technical_skills).intersection(raw_skills))[:40]\n    print(\"Top technical skills\",top_technical_skills)\n    \n    ## Extract adjectives\n    \n    # Delete technical skills from raw skills list\n    ## At this steps, raw skills list doesnt contain the technical skills\n    raw_skills = [x for x in raw_skills if x not in top_technical_skills]\n    raw_skills = list(set(raw_skills) - set(top_technical_skills))\n\n    # transform list to string\n    txt = \" \".join(raw_skills)\n    blob = TextBlob(txt)\n    #top 6 adjective\n    top_adjectives = [w for (w, pos) in TextBlob(txt).pos_tags if pos.startswith(\"JJ\")][:40]\n    print(\"Top 6 adjectives: \",top_adjectives)\n    \n    output = output.append({'job_title':clf.classes_[i],\n                        'technical_skills':top_technical_skills,\n                        'soft_skills':top_adjectives },\n                       ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:30:55.761972Z","iopub.execute_input":"2024-01-13T10:30:55.762393Z","iopub.status.idle":"2024-01-13T10:30:59.223461Z","shell.execute_reply.started":"2024-01-13T10:30:55.762355Z","shell.execute_reply":"2024-01-13T10:30:59.221793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Correlation between jobs and skills:","metadata":{}},{"cell_type":"code","source":"# print(output.T)\nfor i in output['soft_skills']:\n    print(i)d","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:30:59.225464Z","iopub.execute_input":"2024-01-13T10:30:59.225986Z","iopub.status.idle":"2024-01-13T10:30:59.23501Z","shell.execute_reply.started":"2024-01-13T10:30:59.225934Z","shell.execute_reply":"2024-01-13T10:30:59.233707Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"We reached acceptable accuracy with the basic model Naive Bayes.\n\nThis solution can be improved by:\n* adding a larger dataset and thus a larger training data for naive bayes algorithm\n* Extracting more accurate adjectives: library TextBlob that we used for this exercice has some inaccuracies when extracting adjectives. For example, it faulty considered the terms \"app\", \"web\" \"test\" as adjectives.\n* Experimenting with other models for better model accuracy score\n* Using bi-grams tokens and not only uni-grams ones. \n* Using technologies such as pyspark to make the data manipulation pipeline more scalable\n* Adding an exhaustive list of technologies","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}